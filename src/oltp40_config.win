####################################################################
# OLTP-EMUL test for Firebird database - configuration parameters.
# To get last version type following command:
# git clone https://github.com/FirebirdSQL/oltp-emul .
# This file is used for launching test and ISQL sessions on WINDOWS host
# with running Firebird 4.x 
# Parameters are extracted by '1run_oltp_emul.bat' command scenario.
####################################################################


#::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR START AND FINISH ISQL SESSIONS
#::::::::::::::::::::::::::::::::::::::::::::::::


   # Folder with Firebird console utilities (isql, fbsvcmgr, gbak, gstat).
   # Trailing backslash is optional.
   # Allows referencing to existing OS environment variable by using exclamation sign.
   # Example:
   # fbc = C:\Firebird40
   #
   # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters.
   # If FB binaries are in directory like "C:\Program files (x86)\Firebird Database Server" 
   # then make copy of this folder to something like C:\firebird.
   #
   fbc = C:\Firebird40



   # Alias or full path and file name of database.
   # If you want this database be created by test itself, specify it as
   # FULL PATH and file name. Use only ASCII characters in its name.
   # Allows referencing to existing OS environment variable by enclosing it into exclamation marks.
   # Examples:
   # dbnm = c:\temp\oltp_40.fdb
   # dbnm = !TEMP!\oltp_40.fdb
   #
   # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters.
   #
   dbnm = c:\temp\oltp_40.fdb


   # Backup of DB for storing settings and results of every completed test.
   # When test completes, this .fbk is restored to some temporary .fdb,
   # new data are written there and finally database is backed up again.
   # If absent, then apropriate database will be created on every test launch
   # (see calls of script 'oltp_results_storage_DDL.sql')
   # Scenario 'oltp_overall_report' uses this DB when makes overall report.
   #
   # Must be stored on the same host as <dbnm>.
   # It is recommended to store this DB in the same directory where <dbnm>.
   #
   # NOTE: *BACKUP* must be speficied here rather then .fdb file!
   #
   # Examples:
   # results_storage_fbk = C:\data\oltp_40-results-storage.fbk
   #
   results_storage_fbk = c:\temp\oltp_40_results.fbk


   # External utility to compress HTML report before storing it in the database
   # defined by <results_storage_fbk> parameter.
   # Supported compressors: 7z and zstd.
   # When commented then report will be compressed using standard ZIP format by applying
   # temporary created .vbs scenario using %systemroot%\system32\cscript.exe
   # Extraction of HTML report will be done by 'oltp_overall_report' scenario.
   # Examples:
   # report_compress_cmd=C:\soft\7zip\7za.exe
   # report_compress_cmd=C:\soft\zstd\zstd.exe
   #
   # report_compress_cmd = <no value defined>


   # Parameters for remote connection and authentication.
   # Will be ignored by command scenario if FB runs in embedded mode.
   #
   # Host name or IP address of computer with running Firebird.
   #
   host = localhost


   # Port that is listening by Firebird instance on <host>.
   # In order to check which process is listening now selected port, type (locally on server):
   #     netstat -a -n -o -b -p TCP -p TCPv6 | findstr /i /c:"LISTENING" | findstr /i /c:<port>
   # Output will contain PID of process at last token. Put this value in the command:
   #     wmic process where "ProcessID=<PID>" get ExecutablePath /format:list
   #
   port = 3050


   # Login for connect to FB services and database. Account must have the same rights as SYSDBA.
   usr = SYSDBA


   # Password for <usr>
   pwd = masterkey


   # Folder for storing .sql scenarios, STDOUT and STDERR logs of every working isql session.
   # Trailing backslash is optional.
   # Allows referencing to existing OS environment variable by enclosing it into exclamation marks.
   # Examples:
   # tmpdir = c:\temp\logs.oltp40
   # tmpdir = !temp!\logs.oltp40
   #
   # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters.
   # If your TEMP variable is like "C:\Documents and Settings\User\Local Settings\Temp"
   # then do NOT use here reference to it (!temp!) and specify something like C:\TEMP instead.
   #
   tmpdir = c:\temp\logs.oltp40


   # Condition for removing or preserving ISQL logs in <tmpdir> after test finish.
   # Possible values: always | never | if_no_severe_errors
   # * 'always' means that logs will be removed after test finish regardless any result.
   # * 'never' means that logs will be always preserved.
   # * 'if_no_severe_errors' means that logs will be removed only if no severe exceptions occured during test.
   # Test considers following exceptions as 'severe':
   #    gdscode   description
   #  ---------   ------------
   #  335544321   string truncation: attempt to assign too long text into string variable.
   #  335544347   not_valid: validation error for column.
   #  335544558   check_constraint: operation violates CHECK constraint on view or table.
   #  335544665   unique_key_violation: operation violates PRIMARY or UNIQUE KEY constraint
   #  335544349   no_dup: operation violates unique index
   #  335544466   foreign_key: violation of FOREIGN KEY constraint.
   #  335544838   foreign_key_target_doesnt_exist: attempt to insert/update field in child table with value which does not exists in parent table
   #  335544839   foreign_key_references_present: attempt to delete parent record while child records exist and FK was declared without CASCADE clause
   #
   # NOTE: if FB crash occures during test run then value of this parameter will be ignored and all logs will be preserved.
   # Recommended value: if_no_severe_errors
   #
   remove_isql_logs = if_no_severe_errors


#:::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR REPLICATION, WORKLOAD LEVEL AND PAUSES
#:::::::::::::::::::::::::::::::::::::::::::


   # If you plan this database be involved in replication, then one need to add primary keys
   # to all persistent tables that can be changed during test work.
   # Assign value of following parameter to 1 in order all necessary changes be added to tables DDL
   # (primary keys and triggers for some of tables).
   # Setting value to 0 will DROP all changes that are unneeded when test runs without replication.
   # This parameter can be changed 'on the fly': database recreation is NOT required, but in case
   # when database has valuable size, changes will be applied not instantly.
   #
   used_in_replication = 0


   # Test has several settings that define how much work should be done by each business action in average.
   # All of them are considered as separate enumerations: when new ISQL session creates connection, it reads
   # "entry" setting about selected workload level and then read all other settings for THIS workload level.
   # Parameter 'working_mode' is mnemonic for these enumerations. Possible values for this parameter are:
   # SMALL_01, SMALL_02, SMALL_03, MEDIUM_01, MEDIUM_02, MEDIUM_03, LARGE_01, LARGE_02, LARGE_03 and HEAVY_01
   # In case of launching test from several machines ensure that all of them have the same value for this parameter.
   # Completely new workload mode can be added to the test by editing file 'oltp_main_filling.sql', see there
   # sub-section "Definitions for workload modes".
   # WARNING: exception will raise on test startup if this value was mistyped and has no corresponding data in DB.
   # CAUTION: assigning LARGE* or HEAVY* modes leads to extremely high workload! Do this only when you have really
   # powerful server with lot of CPUs, huge RAM and very fast I/O system.
   #
   # Mnemonic name of workload mode (must be specified without quotes, case-insensitive):
   #
   working_mode = small_03


   # This parameter defines volume of work that will be done by each ISQL before it will detach from DB and reconnect.
   # Normally in a production system frequency of reconnections must be low.
   # Rather, each connection must do as much work as possible. Unfortunately, when ISQL does its work by executing
   # script, one can not to check log of errors which occured during this execution. Some errors can require test
   # to be prematurely stopped (e.g. if FB process crashed and it was reflected in firebird.log). But each session
   # can found this only when ISQL finished. This mean that value of following parameter must belong to reasonable scope.
   # For some (exotic) purpoces when it is needed to increase frequency of reconnections one may to set it to 5...50.
   # Recommended value: 300
   #
   actions_todo_before_reconnect = 300


   # Maximal number connections per second at the test startup phase.
   # Defines allowed rate of new attachments appearance for making workload grow smoothly.
   #
   # We have to limit RATE of requests for new attachments, especially when total count of launching ISQL sessions 
   # is 1000 or more. Otherwise some of sessions will get failure on attempt to establish connection with text:
   #     Statement failed, SQLSTATE = 08004
   #     connection rejected by remote interface
   # If value of this parameter less than 10 or greater than 100 then delays will not occur and all sessions will try to establish
   # their attachments at the same time. This can be reason of "connection rejected" error.
   # Otherwise:
   # * if number of sessions not exceeds [max_cps] then delay will be evaluated as random value between 1 and 4 seconds.
   # * if number of sessions greater that this parameter then delay will be evaluated as: 1 + session_id / <max_cps>
   #
   # NOTE. If you intend to launch more than 1000 sessions then consider to adjust Windows registry settings:
   # * goto HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\TCPIP\Parameter
   # * create a new REG_DWORD value with name: TcpTimedWaitDelay. Set it to 60.
   # * create a new REG_DWORD value with name: MaxUserPort. Set it to 32768 (this defines ephemeral port range).
   #
   # Recommended value: 20...30
   #
   max_cps = 25


   # MINIMAL pause duration between each business operations, in seconds.
   # This parameter has default value 0 and can be commented.
   # Duration of pause will be evaluated at runtime as random value 
   # between %sleep_min% and %sleep_max% values.
   #
   sleep_min = 0


   # MAXIMAL pause duration between each business operations, in seconds.
   # Default: 0 - no pauses, next transaction will start immediatelly after previous commit.
   # This leads to maximal (non-realistic) level of workload.
   # Delay statement will be inserted in .sql script by '1run_oltp_emul'; the form of this statement depends on value of
   # parameter 'sleep_ddl':
   # 1) If parameter 'sleep_ddl' is commented (undefined) then temporary .vbs script will be created in the <tmpdir> folder
   #    and Windows built-in utility 'cscript.exe' will be invoked with passing required delay to this .vbs:
   #    %systemroot%\system32\cscript.exe //nologo //e:vbscript //t:NNN <tmpdir>\<tmp_vbs> <sleep_max>
   #    This leads to excessive OS workload when number of sessions is more than ~300.
   # 2) Otherwise special UDF for delay will be invoked from separate execute block after each transaction commit.
   #    This UDF must be declared in SQL-script which name is defined by <sleep_ddl> parameter (see this config).
   #
   # NOTE. THIS PARAMETER IS MANDATORY AND CAN NOT BE COMMENTED.  SPECIFY 0 IF NO PAUSES REQUIRED.
   #
   sleep_max = 0


   # When we want to insert delays between subsequent business actions then parameter sleep_max > 0 must be specified.
   # Delays can be done either by using calls of external OS command (i.e. "shell ... ;") or by UDF invocation.
   # Calls to external OS command from dozen of sessions leads to valuable load, especially when number of sessions more than 300.
   # This can be avoided if delays are done via UDF calls.
   #
   # Parameter 'sleep_ddl' specifies mame of .sql script with declaration of UDF for DELAYS between subsequent business actions.
   # Script must correctly drop old UDFs with any name that contains phrases: DELAY, SLEEP or PAUSE.
   # After dropping, script must create new UDF and test it (for checking results in log).
   # This script will be applied only when parameter 'sleep_max' greater than 0.
   # Note. The whole following phrase:
   #
   #     declare external function <UDF_name>
   #
   # -- must be written on the SINGLE LINE in this script ( <UDF_name> will be searched in this line as its 4th word).
   #
   # This UDF implementation (.dll file) must be stored in the server-side folder, usually in %FIREBIRD_HOME%\UDF.
   # Also, UDF calls must be enabled in firebird.conf (e.g.: UDFaccess = restrict UDF)
   # See description for parameter 'UDFaccess' in standard firebird.conf for details.
   # Test provides its own UDF and appropriate declaration script named: 'oltp_sleepUDF_win.sql'.
   # Unpack file .\util\udf64\SleepUDF.dll.zip and put file SleepUDF.dll in any folder that is allowed by
   # 'UDFaccess' parameter from firebird.conf.
   # NOTES.
   # You can leave parameter 'sleep_ddl' commented if 'mon_unit_perf' is not 2.
   # UDF usage is mandatory when config parameter 'mon_unit_perf' has value 2.
   #
   # By default, UDF usage is deprecated in Firebird 4.0+ and parameter UDFacces and absent in its firebird.conf.
   # You have to add it or uncomment manually.
   #
   sleep_ddl = .\oltp_sleepUDF_win.sql


   # Should SET TRANSACTION statement include NO AUTO UNDO clause. Avaliable values: 1=yes, 0=no
   # Performance can be increased if this option is set to 1:
   # SuperServer:   5 -  6 %
   # SuperClassic: 10 - 11 %
   # Recommended value: 1
   #
   no_auto_undo = 1


   # Minimal interval, in minutes, between two subsequent calls of service procedure 'srv_recalc_idx_stat' which updates index statistics.
   # Only indexes for tables that are participated in most often performing queries are affected. 
   # Note that frequent update of index statistics has sense only for small databases which have quickly changed data distribution.
   # There is no sense to update index statistics if test will runs for 1-2 hours and database has size more than 100 Gb: most probably
   # it will finish at the moment when test itself will also be close to expiration. 
   # In that case set value of this parameter to zero to prevent selection of procedure that does this updating.
   #
   # Recommended value of this parameter depends on size of database:
   # within scope 30...60 minutes for databases with size up to 20 Gb;
   # within scope 60...90 minutes for databases with size 20...40 Gb;
   # within scope 90...120 minutes for databases with size 40...60 Gb;
   # within scope 120...240 minutes for databases with size 60...80 Gb;
   # 0 (zero) for databases with size more than 80...100 Gb (this means that statistics will not be updated at all).
   # NOTE. For big databases (with size more than 100 Gb) updating of index statistics has sense only for big [test_time] values.
   #
   recalc_idx_min_interval = 30


#:::::::::::::::::::::::::::::::::::
# SETTINGS RELATED TO LOCK CONFLICTS
#:::::::::::::::::::::::::::::::::::


   # When some session must change exicting document (rather than to create new), it chooses it using random selection.
   # This can lead to lot of UPDATE CONFLICTS between concurrent sessions, especially when number of documents is small.
   # Also, even when two sessions choose different documents but at least one of wares is the same, business actions can 
   # lead stock remainder for such ware become zero.
   # Further attempts to withdraw this ware lead to exception referring to inadmissible negative remainder.
   # This means that all previous work of this transaction was in vain and it has to rollback changes.
   #
   # We can separate sessions in such way that each of them will work within "sandbox" and never fall in conflict with
   # concurrent sessions for documents or stock remainders.
   #
   # Assign 1 to this parameter if you want to separate work of sessions and thus totally exclude exceptions related to
   # update conflicts and violation or check constraint defined for aggregated remainders value.
   # Otherwise set it to 0.
   # Recommended value: 1
   #
   separate_workers = 1


   # How many documents from other's "sandboxes" can be taken in processing by 'this' ISQL session, percent.
   # Value 0 means that we do not allow ISQL session to take any documents except those which was created by itself.
   # Value 100 means that we require for each ISQL session take for processing only OTHER's documents. 
   # Moreover, this also mean that we want ISQL session 'forget' about documents which were created by itself.
   # This will lead to extremely high number of lock-conflicts and very poor performance.
   # Recommened value: 0 - for benchmark purposes; 30...50 - for investigations.
   #
   update_conflict_percent = 0


   # How business operations should be selected: randomly or in predictable manner.
   # Allowed values:
   # random - on occasional basis, but with respect to priority/probability of business operatrions nature;
   # predictable - forcedly make every ISQL session to work using given sequence of business operations, i.e.
   #     create client order -> create order to supplier -> get invoice from supplier -> ...
   # Value 'predictable' was not deeply tested and currently can lead to poor performance.
   # See SP 'srv_random_unit_choice' for choising algorithm.
   # Recommened value: random
   #
   unit_selection_method = random


#::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR ADDITIONAL LOGGING 
#::::::::::::::::::::::::::::::::::::


   # Do we add in ISQL logs detailed info for each actions that was registered while current transaction was performed ?
   # Note: value = 1 significantly increases disk I/O on client machine.
   # Do not use it if you are not interested on these data.
   # Recommended value: 0
   #
   detailed_info = 0


   # Setting for enabling queries to monitor tables in order to make detailed performance analysis.
   # When 0 then monitor tables are not queried.
   # When 1 then EVERY session will take two snapshots before and after execution of selected unit.
   # More detailed analysis with detalization down to separate stored procedures can be achieved
   # by updating setting 'mon_unit_list'.
   # When 2 then only ONE session is dedicated to gather monitoring data with obtaining data
   # that relates to ALL other working sessions.
   # It will call special SP 'srv_fill_mon_memo_consumption' every <mon_query_interval>-th second.
   # NOTE: delays for mon_unit_perf=2 between transactions will be done only when 'sleep_ddl' is defined,
   # e.g. when we make delays via UDF, without calls to external OS commands.
   # By default, UDF usage is deprecated in Firebird 4.0+ and parameter UDFacces and absent in its firebird.conf.
   # You have to add it or uncomment manually.
   #
   mon_unit_perf = 0


   # Gathering of monitoring data leads to significant performance penalty if session works as SYSDBA: 
   # all other attachments have to put information about their state into special pool.
   # Benchmarks show that performance can fall for ~10x when all attachments work as SYSDBA and value of
   # parameter <mon_unit_perf> is 1.
   # But actually each worker is interested only about its own data from monitoring rather than others.
   # Behaviour was improved in Firebird 3.x+ for such case: if session works as NON-privileged used then
   # its query to monitoring tables will not affect on other attachments which work under different logins.
   # One can use this improvement and require that test will launch every ISQL session so that it will work
   # with database as non-privileger user, with accessing to DB objects via special role with all needed grants.
   #
   # Parameter 'mon_query_role' specifies name of this role. If it is specified then test will create such role
   # and give it all grants that are needed for normal work. This role will be further granted to all non-privileged
   # users which are also created by test. Parameter 'mon_usr_prefix' must also be specified in this case.
   # If 'mon_query_role' is commented then all sessions will work as SYSDBA.
   # NOTE: actual only for Firebird 3.0 and above. Has no effect on Firebird 2.5.
   # Recommended value: any string that meets FB requirement to the name of ROLE, e.g.: tmp$oemul$worker
   #
   mon_query_role = tmp$oemul$worker


   # Prefix for each name of temporarily created users for work with database.
   # Each user name will be further provided with suffix like '0001', '0002' etc, up to the total number of sessions.
   # These users will be granted to use ROLE which name is defined by <mon_query_role> parameter (see above).
   # After test finish all of them will be dropped.
   # NOTE: actual only for Firebird 3.0 and above. Has no effect on Firebird 2.5.
   # Recommended value: any string that meets FB requirement to the name of USER, e.g.: tmp$oemul$user_
   #
   mon_usr_prefix = tmp$oemul$user_


   # This setting can be used only when config parameter 'enable_mon_query' is 1.
   # List of top-level units (see 'business_ops' table) which performance statistics we want 
   # to be logged by querying  monitoring tables. Logging is done by SP srv_log_mon_for_traced_units.
   # Value can be single unit name or LIST of unit names delimited by forward slash.
   # Example:
   #     sp_make_qty_storno/sp_kill_qty_storno/sp_multiply_rows_for_qdistr/sp_multiply_rows_for_pdistr
   # Default value: // (two slashes) without any characters between them, i.e. no interested units for logging.
   #
   mon_unit_list = //


   # This setting can be used only when config parameter 'enable_mon_query' is 2.
   # Number of seconds between calls to SP that gathers monitoring data for all working attachments.
   # Monitor data will be gathered only by single (dedicated) isql session which is launched first.
   # Parameter 'sleep_ddl' must be uncommented and its value has to point on existent SQL script
   # with UDF declaration that implements delay.
   # Actual duration of delay, in seconds, will be evaluated as minimal of <mon_query_interval>
   # and <test_time> * 60 divided by 20.
   #
   mon_query_interval = 60


   # Should 1st of launching ISQL instances also start asynchronously FBSVCMGR with opening TRACE session ?
   # If yes then final report will have result of parsing trace log for that ISQL session activity with
   # aggregate data about each business action. Also, average values will be calculated for:
   # 1) speed of fetches and marks per second;
   # 2) ratios reads / fetches and writes / marks.
   # These values will be divided for 10 equal time intervals in order to see changes 
   # in performance that could occur during <test_time> phase of test.
   # There is no performance penalty when single trace session is active so one may safely
   # to set this value to 1. Note though that if you will interrupt test by brute kill all
   # ISQL sessions than you have also to kill process of FBSVCMGR which can remain active.
   # Currently implemented only for Windows.
   #
   trc_unit_perf = 0


#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS TO FORCEDLY TERMINATE OF WORK BEFORE THE DUE TIME
#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


   # Mnemonics of exceptions which must force test to be stopped (see calls of fn_halt_sign(gdscode)):
   # 'CK' -- halt if CHECK violation or 'not_valid' occurs (mostly this can be due to negative stock remainders)
   # 'PK' -- halt if PK or UK violation occurs
   # 'FK' -- halt if FK violation occurs // now n/a because test does not use foreign keys.
   # 'ST' -- halt if gdscode 335544842 appeared at the top of stack and logged into perf_log (strange problem only in 3.0 SC)
   # These mnemonics can be combined in list, i.e.: 'CK/PK/FK' - halt if CHECK or PK or FK violation occurs
   # Default: '/CK/' ==> force test to be stopped on attempt to write NEGATIVE values for stock remainders.
   # 12.02.2015: PK and FK violations *can* be detected only in sp_make_qty_storno & sp_kill_qty_storno,
   # but it is due to undefined order of UNDO actions inside the engine when some action must be cancelled.
   # Detailed investigation:
   # sql.ru/forum/1142271/posledstviya-nepredskazuemo-neposledovatelnyh-otkatov-izmeneniy-pri-exception
   # Explanation by dimitr was sent privately to e-mail, letters date = 12.02.2015.
   #
   halt_test_on_errors = /CK/


   # How stock remainders should be verified BEFORE totalling turnovers (see procedure 'sp_make_invnt_saldo').
   #
   # Declarative CHECK constraint for non-negative QTY_* columns should NOT ever be fired in this test.
   # This parameter defined numeric value which bits must be interpreted as:
   # bit#0 := 1 -- perform calls of procedure SRV_FIND_QD_QS_MISM in order to register mismatches between
   #               doc_data.qty and total number of rows in QDISTR and QSTORNED tables for doc_data.id;
   # bit#1 := 1 -- perform calls of procedure SRV_CHECK_NEG_REMAINDERS instead of actual totalling turnovers
   #               to the table INVNT_SALDO. This value must be used only for debug purposes.
   # bit#2 := 1 -- allow dump dirty data into debug tables for analysis, see sp ZDUMP4DBG, in case
   #               when PK/FK or check constraint is violated (see also parameter 'halt_test_on_errors')
   #               NOTE: when bit#2 has value 1 then parameter 'create_with_debug_objects' must be 1
   #               to force build scenario create auxiliary Z-tables.
   #
   # This parameter was used during test development and can be useful in case of some changes/refactoring
   # in test logic. Normally its value must be 1.
   #
   qmism_verify_bitset = 1


   # Parameter 'use_external_to_stop' defines name of text file that can be used for premature stop all working isql sessions.
   # This parameter is NOT required, i.e. it can be commented. In this case test can be stopped by running temporary script
   # '$tmpdir/1stoptest.tmp.sh' which is created every time when test starts by script '1run_oltp_emul.sh'.
   # Usage of this script is OK for most cases except extremely high workload when establishing of new connect will be unavaliable.
   #
   # When extremely high workload is used then following message can appear on every attempt to establish new attachment:
   #     Statement failed, SQLSTATE = 08004
   #     connection rejected by remote interface
   #
   # In such case it can be more reliable to use EXTERNAL TABLE (i.e. TEXT FILE) to make all attachments to stop their work. 
   # This is so because every running session 'looks' from time to time into this external table and checks existense of at 
   # least one record in it. So, test will be quickly self-stopped when at least one non-empty line exists there. 
   # Please note that you have to make this file EMPTY before every new test run. Test can not do that when server is remote.
   #
   # If you have decided to use EXTERNAL FILE then following steps must be done for premature terminate all test activity:
   #   1. Open that file in text editor and type one ascii-character there;
   #   2. Press ENTER and save this file.
   #   3. Make this file empty again when all isql sessions terminated their work.
   # Also, please note on value of parameter "ExternalFileAccess" in firebird.conf:
   #   1. When ExternalFileAccess = FULL then 'use_external_to_stop' must be full path and name of text file that will be 
   #      queried by every attachment as 'stop flag'.
   #   2. When ExternalFileAccess = RESTRICTED then 'use_external_to_stop' must be only NAME of file, without path.
   #
   # By default this parameter is UNDEFINED, i.e. only temporary batch can be used for stop test prematurely:
   #
   #     <tmpdir>\1stoptest.tmp.bat 
   #
   # use_external_to_stop = <no value defined>


#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR DATABASE CREATION PROCESS AND INITIAL DATA FILLING 
#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    #
    # *** NOTE *** 
    # Following settings except 'create_with_fw' and 'create_with'sweep'
    # will be IGNORED if database exists and has required number of documents.
    # If parameter 'host' is one of: {'localhost', '127.0.0.1'} then current
    # values of 'create_with_fw' and 'create_with'sweep' will be applied to DB
    # every time before launching ISQL sessions.


   # Setting for FORCED WRITES attribute which must be written in the DB header before all sessions launch.
   # Value can be one of: sync | async
   # RECOMMENDED value:
   #     sync - if you want to test performance for database that is not involved in replication;
   #     async - if you plan to use DB which will be replicated to other host.
   #             (note that in such case you must set parameter 'used_in_replication' to 1).
   # When value is 'async', ensure that firebird.conf contains following uncommented parameters:
   #     MaxUnflushedWrites = -1
   #     MaxUnflushedWriteTime = -1
   #
   create_with_fw = sync


   # Setting for SWEEP INTERVAL which causes auto sweep start.
   # Value must be not less than -1.
   # Value -1 means that default value (20000) will be written into DB header.
   # Sweep starts when OST-OIT more than this threshold. Value 0 disables sweep.
   #
   # RECOMMENDED value for create_with_sweep is 0 (zero).
   # Sweep start can lead to unpredictable affect on performance, especially for short test duration.
   #
   create_with_sweep = 0


   # Should script be paused if database does not exist or its creation
   # did not finished properly (e.g. was interrupted; 1=yes; 0=no) ?
   # You have to set this parameter to 0 if this batch is launched by 
   # scheduler on regular basis. Otherwise it is recommended to set 1.
   #
   wait_if_not_exists = 0


   # Should script be paused after creation database objects before starting
   # initial filling with <init_docs> documents (mostly need only for debug; 1=yes, 0=no) ?
   #
   wait_after_create = 0


   # Number of documents, total for all their types, needed for initial data population.
   # Command scenario will compare number of existing document with this
   # and create new ones only if {init_docs] still greater than obtained.
   #
   # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 0 (ZERO) ***
   #
   # Instead of generating dosuments by single attachment it is much more properly (and faster) to assign some big value 
   # to 'warm_time' parameter (say, 1440 which means to run for 1 day) and also set 'test_time' to 0 (zero). 
   # When size of database will reach value that you consider as enough then just stop the test by running batch 
   # '!tmpdir!\1stoptest.tmp.bat' which always is created when test starts.
   #
   init_docs = 0


   # This parameter actual only when 'init_docs' greater than 0 and 'separate_workers' is 1, i.e. when you want to separate 
   # each ISQL session in such way that they will not ever meet update conflicts during work. In other cases value if this
   # parameter is ignored.
   # If you decide to generate initial quantity of documents by using old 'init_docs' value then assign to 'expected_workers' 
   # value that is equal to the  number of ISQL sessions that is expected to run.
   #
   expected_workers = 100


   # Actual only when 'init_docs' greater than 0 and FB mode is Classic Server or SuperClassic. Will be ignoired in SuperServer.
   # Number of pages for usage during init data population ("-c" switch for ISQL). Used ONLY during phase of initial data population.
   # Make sure that it is LESS than FileSystemCacheThreshold, which default is 65536.
   #
   # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 10000 ***
   #
   init_buff = 4096


   # Actual only when 'init_docs' greater than 0
   # Should command scenario (1run_oltp_emul) be PAUSED after finish creating
   # required initial number of documents (see parameter 'init_docs'; 1=yes, 0=no) ?
   # Value = 1 can be set if you want to make copy of .fdb and restore later
   # this database to 'origin' state. This can save time because of avoiding need
   # to create [init_docs] again:
   #
   wait_for_copy = 0


   # Do we want to create some DEBUG objects (tables, views and procedures)
   # in order to:
   # 1) make dumps of all data from tables when critical error occurs;
   # 2) make miscelaneous diagnostic queries via "Z_" views.
   # Value=1 will cause "oltp_misc_debug.sql" be called when build database.
   # NB: setting 'QMISM_VERIFY_BITSET' must have bit #2 = 1 when this value = 1.
   # (see oltp_main_filling.sql)
   # Recommended value: 1
   #
   create_with_debug_objects = 1


   # Test has two tables which are subject of very intensive modifications: QDistr and QStorned.
   # Performance highly depends on time which engine spends on handling DP, PP and index pages
   # of this tables - they are "bottlenecks" of schema. Database can be created either with two
   # these tables or with several "clones" of them (with the same stucture). The latter allows
   # to "split" workload on different areas and reduce low-level lock contention.
   # Should heavy-loaded tables (QDistr and QStorned) be splitted on several different tables,
   # each one for separate pair of operations that are 'source' and 'target' of storning ?
   # Avaliable values: 
   # 0 = do NOT split workload on several tables (instead of single QDistr and QStorned);
   # 1 = USE several tables with the same structure in order to split heavy workload on them.
   #     NOTE (2019). Not only Qdistr and QStorned but also PERF_LOG table will be 'splitted' onto
   #     several tables (with names PERF_SPLIT_01...PERF_SPLIT_09) when this parameter is set to 1. 
   # Recommended value: 1.
   #
   create_with_split_heavy_tabs = 1


   # Whether heavy-loaded table (QDistr or its XQD_* clones) should have only one ("wide")
   # compound index or two separate indices (1=yes, 0=no).
   # Number of columns in compound index depends on value of two parameters:
   # 1) create_with_split_heavy_tabs and 2) create_with_separate_qdistr_idx (this).
   # Order of columns is defined by parameter 'create_with_compound_idx_selectivity'.
   # Recommended value: 0.
   #
   create_with_separate_qdistr_idx = 0


   # Parameter 'create_with_compound_columns_order' defines order of fields in the starting part
   # of compound index key for the table which is subject to most heavy workload - QDistr. 
   # Avaliable options: 'most_selective_first' or 'least_selective_first'.
   # When choice = 'most_selective_first' then first column of this index will have selectivity = 1 / [W],
   # where [W] = number of rows in the table 'WARES', depends on selected workload mode.
   # Second and third columns will have poor selectivity = 1/6.
   # When choice = 'least_selective_first' then first and second columns will have poor selectivity = 1/6,
   # and third column will have selectivity = 1 / [W].
   #
   # Actual only when create_with_split_heavy_tabs = 0.
   # Recommended value: most_selective_first
   #
   create_with_compound_columns_order = most_selective_first


#:::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR SCHEDULED-BASIS JOB AND TEST REPORT
#:::::::::::::::::::::::::::::::::::::::::::::::::::::


   # Number of minutes since test launch for which evaluation of performance score is omited because database is 'cold' (not in cache).
   # Means the same as 'ramp-up' period in TPC-C specification: we have to allow all sessions to establish  attachments and read some
   # data into Firebird page cache.
   # Recommended value: DBSize_Gb/2, where DBSize_Gb is size of database in Gb, but not less than 30 minutes.
   # To estimate whether value of this parameter is apropriate, run test for 2-3 hours and look after its finish in report 
   # "Performance per minute". Performance counter at the end of <warm_time> period must be close to values for subsequent 20-30 minutes.
   # See also TPC-C specification rev 5.11:
   # * 5.6.4 (page 78) - graphical explanation of ramp-up period;
   # * Appendix C (page 132) - numerical quantities summary.
   #
   warm_time = 30


   # Duration of main test phase which starts after 'ramp-up'. Means the same as 'measurement interval' in TPC-C specification.
   # Overall performance score is evaluated as total number of successfully completed transactions during this phase divided by <test_time>.
   # At the end of this phase test will stop itself, i.e. you do not have to interrupt ISQL sessions.
   # Note that TPC-C requires minimum 120 minutes for this phase, but your system must allows to run test during 480 minutes - and this
   # value does not include <warm_time> phase (see TPC-C rev 5.1, 5.5.2.1, page 75).
   # ATTENTION. Reports and performance score for test_time less than 120 minutes must be considered as unreliable (doubtful).
   # Recommended value: at least 180.
   #
   test_time = 180


   # OBSOLETE. WILL BE REMOVED LATER.
   # This parameter used earlier for one of reports which was removed.
   # Currently its value will be ignored.
   #
   test_intervals = 30


   # This parameter is used in 'oltp-scheduled' scenario and points to the name of etalone DB which serves
   # as source for copy to work DB before every new test starts.
   # It is possible to get following error when DB was moved from one host to another without b/r:
   #     Statement failed, SQLSTATE = 22021
   #     COLLATION NAME_COLL for CHARACTER SET UTF8 is not installed
   # In this case try following command:
   #     <fbc>\gfix.exe -icu <etalon_dbnm>
   # NOTE.
   # It is recommended to change state of this DB to 'full shutdown' or at least make it read only.
   #
   etalon_dbnm = c:\temp\oltp_40.etalone.fdb


   # Create report in HTML format (along with plain text) ? Avaliable options: 1 = yes, 0 = no.
   # NOTE: when this parameter is 1, time of reports creation will be slighly increased.
   #
   make_html = 1


   # Should DB statistics be included into final report ? Avaliable options: 1 = yes, 0 = no.
   # When this parameter is 1, statistics output is parsed in order to get data about amount of record versions 
   # and maximal versions for each table. Final report will contain auxiliary table with aggregated info about versions.
   # WARNING. This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
   #
   run_db_statistics = 0


   # Should online validation be done after test finish ? Avaliable options: 1 = yes, 0 = no.
   # Result of validation will not include messages about passed pointer pages in order to make report shorter.
   # WARNING. This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
   #
   run_db_validation = 0


   # Optional parameter.
   # Should final report be saved in file with name which contain info about FB, database, test settings ?
   # If no, leave this parameter commented. In that case final report will be always saved with the same name.
   # If yes, choose format according to one of following:
   #     regular   - appropriate for quick found performance degradation, without details of test settings
   #     benchmark - appropriate for analysis when different settings are applied
   #
   # Example of report name when this parameter = 'regular':
   #     YYYYmmDD_HHMM_score_07011_build_2241_ss40__3h00m_100_att_fw__on.txt
   # Example of report name when this parameter = 'benchmark':
   #     ss40_fw_off_split_most__sel_1st_one_index_score_07989_build_2241__3h00m_100_att_YYYYmmDD_HHMM.txt
   #     (where 'YYYYmmDD_HHMM' is timestamp of test start)
   #
   # Available options when uncommented: regular | benchmark
   #
   file_name_with_test_params = regular


   # Suffix for adding at the end of report name. CHANGE this value to some useful info about host location, 
   # hardware specifics, FB instance etc.
   # For example, use value of %COMPUTERNAME% environment variable
   # It is also useful to specify here number of CPU cores and size of RAM, e.g.:
   #     testsrv_cpu16_ram64
   #
   file_name_this_host_info = !COMPUTERNAME!


   # Do we want to include in the report details about server hardware and OS ?
   # Avaliable options: 1 = yes, 0 = no.
   # This setting has sense only when you launch ISQL sessions at the server which you are 
   # interesting on, i.e. when value of 'host' parameter is localhost or 127.0.0.1
   #
   gather_hardware_info = 1


#::::::::::::::::::::::::::::::::::::::::::::
#  EXOTIC SETTINGS (USAGE WAS NOT CHECKED)
#::::::::::::::::::::::::::::::::::::::::::::


   # Do we use mtee.exe utility to provide timestamps for error messages 
   # before they are logged in .err files (1=yes, 0=no) ?
   # Windows only. Not implemented for Linux, will be ignored at runtime.
   #
   use_mtee = 0


   # Does Firebird running in embedded mode ? (1=yes, 0=no)
   #
   is_embed = 0


