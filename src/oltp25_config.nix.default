##########################################################################################
# OLTP-EMUL test for Firebird database - configuration parameters.
# This config file is used for launching test and ISQL sessions
# on POSIX host with running Firebird 2.5.
# Parameters are extracted by '1run_oltp_emul.sh' command scenario.
#
# To get last version of test use following command:
#
# git clone --config core.autocrlf=false https://github.com/FirebirdSQL/oltp-emul .
#
##########################################################################################

#::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR START AND FINISH ISQL SESSIONS
#::::::::::::::::::::::::::::::::::::::::::::::::

    # Folder with Firebird console utilities (isql, fbsvcmgr).
    # Trailing backslash and double quotes are optional.
    # Allows referencing to existing OS environment variable by using dollar sign.
    # Note that path to these utilities can differ from standard ('/opt/firebird/bin')
    # when installing Firebird in OS repository. For Ubuntu/Debian it is: '/usr/bin'.
    # Samples:
    # fbc = /opt/firebird/bin
    # fbc = /usr/bin
    # fbc = $FB25_HOME/bin
    #
    fbc = /opt/firebird/bin

    # POSIX only, optional: command-line utility for operate as ISQL.
    # Actual for builds from OS repository, e.g. 'isql-fb' for Firebird in Ubuntu/Debian.
    # Full name of this utility will be evaluated as concatenation of $fbc and $clu values.
    # Value 'isql' will be used if you leave this parameter commented.
    #
    # clu = isql


    # Alias or full path and file name of database.
    # If you want this database be created by test itself, specify it as
    # FULL PATH and file name. Use only ASCII characters in its name.
    # Allows referencing to existing OS environment variable by using dollar sign.
    # Use forward slash ("/) in all cases, even when database is on Windows host.
    # Sample:
    # dbnm = /var/db/oltp25_test.fdb
    #
    dbnm = /var/db/oltp25_test.fdb


    # Parameters for remote connection and authentication.
    # Will be ignored by command scenario if FB runs in embedded mode.
    #
    host = localhost
    port = 3050
    usr =  SYSDBA
    pwd =  masterkey


    # Folder where to store logs of STDOUT and STDERR redirection for each ISQL session.
    # Trailing backslash is optional.
    # Allows referencing to existing OS environment variable by using dollar sign.
    # Firebird account must have 'r' and 'w' file permissions in this folder.
    # Samples:
    # tmpdir = /var/tmp/logs.oltp25_emul
    # tmpdir = $TMP/logs.oltp25
    #
    tmpdir = /var/tmp/logs.oltp25


    # Condition where all ISQL logs should be removed after test will finish.
    # Possible values: always | never | if_no_severe_errors
    # Option 'always' means that ISQL logs in %tmpdir% will be removed after test finish without any condition.
    # Option 'never' means that ISQL logs will be preserved.
    # Option 'if_no_severe_errors' means that ISQL logs will be removed if no severe exceptions occured.
    # Test considers following exceptions as 'severe':
    #   335544558 check_constraint     (Operation violates CHECK constraint @1 on view or table @2).
    #   335544347 not_valid            (Validation error for column @1, value "@2").
    #   335544665 unique_key_violation (Violation of PRIMARY or UNIQUE KEY constraint "..." on table ...") - if table has unique CONSTRAINT
    #   335544349 no_dup               (attempt to store duplicate value (visible to active transactions) in unique index "***") - if table has only unique INDEX

    # Recommended value: if_no_severe_errors

    remove_isql_logs = if_no_severe_errors


#::::::::::::::::::::::::::::
#  SETTINGS FOR TEST DURATION
#::::::::::::::::::::::::::::

    # Number of minutes since test launch for which evaluation of performance score is omited because database is 'cold' (not in cache).
    # Means the same as 'ramp-up' period in TPC-C specification: we have to allow all sessions to establish  attachments and read some
    # data into Firebird page cache.
    # Recommended value: DBSize_Gb/2, where DBSize_Gb is size of database in Gb, but not less than 30 minutes.
    # You can estimate whether value of this parameter is apropriate if run test for 2-3 hours and look after its finish in report 
    # "Performance per minute". Performance counter at the end of <warm_time> period must be close to values for subsequent 20-30 minutes.
    # See also TPC-C specification rev 5.11:
    # * 5.6.4 (page 78) - graphical explanation of ramp-up period;
    # * Appendix C (page 132) - numerical quantities summary.
    #
    warm_time = 30


    # Duration of main test phase which starts after 'ramp-up'. Means the same as 'measurement interval' in TPC-C specification.
    # Overall performance score is evaluated as total number of successfully completed transactions during this phase divided by <test_time>.
    # At the end of this phase test will stop itself, i.e. you do not have to interrupt ISQL sessions.
    # Note that TPC-C requires minimum 120 minutes for this phase, but your system must allows to run test during 480 minutes - and this
    # value does not include <warm_time> phase (see TPC-C rev 5.1, 5.5.2.1, page 75).
    # ATTENTION. Reports and performance score for test_time less than 120 minutes must be considered as unreliable (doubtful).
    # Recommended value: at least 180.
    #
    test_time = 180


#:::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR REPLICATION, WORKLOAD LEVEL AND PAUSES
#:::::::::::::::::::::::::::::::::::::::::::::::::::::


    # Test schema was developed with main goal to provide fastest performance. This required to remove all unneccessary indices.
    # There are several tables which have *no* requirement to be supplied with primary keys (though they have another constraints).
    # But they must be provided with PK if test database will serve as 'master' in your replication schema.
    # Assign value of following parameter to 1 in order all necessary changes (primary keys and triggers for some tables) be added.
    # Setting value to 0 will DROP all changes that are unneeded when test runs without replication.
    # This parameter can be changed 'on the fly': database recreation is NOT required, but changes will be applied not instantly
    # in case when database has valuable size.
    #
    used_in_replication = 0


    # Test has several settings that define how much work should be done by each business action in average.
    # All of them are considered as separate enumerations: when new ISQL session creates connection, it reads
    # "entry" setting about selected workload level and then read all other settings for THIS workload level.
    # Parameter 'working_mode' is mnemonic these enumerations. Possible values for this parameter are:
    # SMALL_01, SMALL_02, SMALL_03, MEDIUM_01, MEDIUM_02, MEDIUM_03, LARGE_01, LARGE_02, LARGE_03 and HEAVY_01
    # In case of launching from several machines ensure that all of them have the same value for this parameter.
    # Completely new workload mode can be added to the test by editing file "oltp_main_filling.sql", see there
    # sub-section "Definitions for workload modes".
    # WARNING: exception will raise on test startup if this value was mistyped and has no correspond data in DB.
    # Mnemonic name of workload mode (must be specified without quotes, case-insensitive):
    #
    working_mode = small_03


    # This parameter defines volume of work that will be done by each ISQL before it will detach from database
    # and create new attach again. Normally in a production system frequency of reconnections must be low.
    # Rather, each connection must do as much work as possible. Unfortunately, when ISQL does its work by executing
    # script, one can not to check log of errors which occured during this execution. Some errors can require test
    # to be prematurely stopped (e.g. if FB process crashed and it was reflected in firebird.log). But each session
    # can found this only when ISQL finished. This mean that value of following parameter must belong to reasonable scope.
    # For some (exotic) purpoces when it is needed to increase frequency of reconnections one may to set it to 5...50.
    # Recommended value: 300
    #
    actions_todo_before_reconnect = 300


    # Maximal number connections per second (max allowed rate of new attachments appearance for making workload grow smoothly).
    # We have to limit RATE of requests for new attachments, especially when total count of launching ISQL sessions 
    # is 1000 or more. Otherwise some of sessions will get failure on attempt to establish connection with text:
    #     Statement failed, SQLSTATE = 08004
    #     connection rejected by remote interface
    # If value of this parameter less than 10 or greater than 100 then delays will not occur and all sessions will try to establish
    # their attachments at the same time. This can be reason of "connection rejected" error.
    # Otherwise:
    # * if number of sessions not exceeds <max_cps> then delay will be evaluated as random value between 1 and 4 seconds.
    # * if number of sessions greater that this parameter then delay will be evaluated as: 1 + session_id / <max_cps>.
    # NOTE. If you intend to launch more than 1000 sessions then consider to adjust following settings in /etc/sysctl.conf:
    # net.core.somaxconn = 2000
    # net.core.netdev_max_backlog = 2000
    # net.core.tcp_max_syn_backlog = 2000
    # net.ipv4.ip_local_port_range = 15000 61000
    # net.ipv4.tcp_tw_reuse = 1
    # net.ipv4.tcp_max_tw_buckets = 1440000
    # See also:
    # http://lxr.linux.no/#linux+v3.2.8/Documentation/networking/ip-sysctl.txt#L111
    # http://lxr.linux.no/#linux+v3.2.8/Documentation/networking/ip-sysctl.txt#L284
    # http://lxr.linux.no/#linux+v3.2.8/Documentation/networking/ip-sysctl.txt#L464
    # https://www.centos.org/docs/5/html/5.1/Deployment_Guide/s3-proc-sys-net.html
    # http://docs.continuent.com/tungsten-clustering-6.0/performance-networking.html
    # https://access.redhat.com/solutions/41776
    #
    # Recommended value: 20...30
    #
    max_cps = 25


    # MINIMAL pause duration between each business operations, in seconds.
    # This parameter has default value 0 and can be commented.
    # Duration of pause will be evaluated at runtime as random value 
    # between $sleep_min and $sleep_max values.
    #
    sleep_min=0


    # MAXIMAL pause duration between each business operations, in seconds.
    # Default: 0 - no pauses, next transaction will start immediatelly after previous commit.
    # This leads to maximal (non-realistic) level of workload.
    # Delay statement will be inserted in .sql script by 1run_oltp_emul.sh; the form of this statement depends on value of
    # parameter 'sleep_ddl':
    # 1) If parameter 'sleep_ddl' is commented (undefined) then OS call ("shell sleep NN;") will ne inserted betwee every adjacent
    #    transactions in order to pause SQL execution. This leads to excessive OS workload when number of sessions is more than ~300.
    # 2) Otherwise UDF will be invoked from separate execute block. Name of this UDF must be declared in the SQL-script defined 
    #    by 'sleep_ddl' parameter.
    # NOTE. THIS PARAMETER IS MANDATORY AND CAN NOT BE COMMENTED.  SPECIFY 0 IF NO PAUSES REQUIRED.
    #
    sleep_max=0


    # Name of SQL script with declaration of UDF that will be used for PAUSES between transactions.
    # An attachment will be IDLE only when <sleep_max> greater than 0 otherwise pauses will not occur at all.
    # Script must correctly drop old existent UDF with any name that contains phrases: DELAY, SLEEP or PAUSE.
    # After dropping, script also must create new UDF and test it (for checking results in log).
    # This script will be applied only when parameter 'sleep_max' greater than 0.
    # Note. The whole following phrase:
    #     declare external function <name>
    # -- must be written on the SINGLE LINE in this script (<name> will be parsed from this line as the 4th word)
    # This UDF implementation (.so file) must be stored in the server-side folder, usually $FIREBIRD_HOME\UDF, which
    # is allowed to be used for UDF calls. See description for parameter 'UDFaccess' in standard firebird.conf for details.
    # Test is provided with its own UDF and appropriate declaration script named "oltp_sleepUDF_nix.sql".
    # Unpack file SleepUDF.so.tar.gz and put file SleepUDF.so in any folder that is allowed by 'UDFaccess' parameter.
    # You can comment this parameter for using only OS external command ('shell sleep ...;') when 'sleep_max' greater than 0.
    #
    sleep_ddl=./oltp_sleepUDF_nix.sql


    # Should SET TRANSACTION statement include NO AUTO UNDO clause (1=yes, 0=no) ? 
    # Performance can be increased if this option is set to 1:
    # SuperServer:   5 -  6%
    # SuperClassic: 10 - 11%
    # Recommended value: 1
    #
    no_auto_undo = 1


    # Minimal interval in minutes between two subsequent calls of service procedure 'srv_recalc_idx_stat' which updates index statistics.
    # Only indexes for tables that are participated in most often performing queries are affected. 
    # Note that frequent update of index statistics has sense only for small databases which have quickly changed data distribution.
    # There is no sense to update index statistics if test will runs for 7-9 hours and database has size more than 100 Gb: most probably
    # it will finish at the moment when test itself will also be close to expiration. 
    # In that case set value of this parameter to zero to prevent selection of procedure that does this updating.
    #
    # Recommended value of this parameter depends on size od database:
    # within scope 30...60 minutes for databases with size up to 20 Gb;
    # within scope 60...90 minutes for databases with size 20...40 Gb;
    # within scope 90...120 minutes for databases with size 40...60 Gb;
    # within scope 120...240 minutes for databases with size 60...80 Gb;
    # 0 (zero) for databases with size more than 80...100 Gb (this means that statistics will not be updated at all).
    # NOTE. For big databases (with size more than 100 Gb) updating of index statistics has sense only for big <test_time> values.
    # See SP 'srv_random_unit_choice' for choising algorithm.
    #
    recalc_idx_min_interval = 0


#:::::::::::::::::::::::::::::::::::
# SETTINGS RELATED TO LOCK CONFLICTS
#:::::::::::::::::::::::::::::::::::

    # Though test uses pessimistic locks (tries to 'catch' document before any further actions) this strategy can not prevent
    # neither from choosing the same document in random algorithm nor from decreasing stock remainder for the same wares.
    # Lot of update conflicts or exceptions related to violate CHECK CONSTRANT about non-negative remainder will occur if
    # we allow sessions to operate with the same ("shared") set of documents. This affects on performance, of course.
    # One may to separate sessions in such way that each of them will work within "sandbox" and never fall in conflict with
    # concurrent sessions for documents.
    # Assign 1 to this parameter if you want to separate work of sessions and thus totally exclude exceptions related to
    # update conflicts and violation or check constraint defined for aggregated remainders value.
    # Otherwise set it to 0.
    #
    separate_workers = 1

    # How many documents from other's sandbox can be taken in processing by 'this' ISQL session, percent.
    # Value 0 means that we do not allow ISQL session to take any documents except those which was created by itself.
    # Value 100 means that we require for each ISQL session take for processing only OTHER's documents. 
    # Moreover, this also mean that we want ISQL session 'forget' about documents which were created by itself.
    # This will lead to extremely high number of lock-conflicts and very poor performance.
    # Recommened value: 0 - for benchmark purposes; 30...50 - for investigations.
    #
    update_conflict_percent = 0

    # How business operations should be selected: randomly or in predictable manner.
    # Allowed values:
    # random - on occasional basis, but with respect to priority/probability of business operatrions nature;
    # predictable - forcely make every ISQL session to work using given sequence of business operations, i.e.
    #     create client order -> create order to supplier -> get invoice from supplier -> ...
    # Value 'predictable' was not deeply tested and currently can lead to poor performance.
    # Recommened value: random
    #
    unit_selection_method = random


#::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR ADDITIONAL LOGGING 
#::::::::::::::::::::::::::::::::::::


    # Do we add in ISQL logs detailed info for each actions that was registered while current transaction was performed ?
    # Note: value = 1 significantly increases disk I/O on client machine.
    # Do not use it if you are not interested on these data.
    # Recommended value: 0
    #
    detailed_info = 0


    # Setting for enabling queries to monitor tables in order to make detailed performance analysis.
    # When 0 then monitor tables are not queried.
    # When 1 then EVERY session will take two snapshots before and after execution of selected unit.
    # More detailed analysis with detalization down to separate stored procedures can be achieved
    # by updating setting 'mon_unit_list'.
    # When 2 then only ONE session is dedicated to gather monitoring data with obtaining data
    # that relates to ALL other working sessions.
    # It will call special SP 'srv_fill_mon_memo_consumption' every 'mon_query_interval'-th second.
    # NOTE: pauses for mon_unit_perf=2 between transactions will be done only when 'sleep_ddl' is defined.
    #
    mon_unit_perf = 0


    # This setting can be used only when config parameter 'enable_mon_query' is 1.
    # List of top-level units (see 'business_ops' table) which performance statistics we want 
    # to be logged by querying  monitoring tables. Logging is done by SP srv_log_mon_for_traced_units.
    # Value can be single unit name or LIST of unit names delimited by forward slash.
    # Sample:
    #     sp_make_qty_storno/sp_kill_qty_storno/sp_multiply_rows_for_qdistr/sp_multiply_rows_for_pdistr
    # Default value: two slashes without any characters between them, i.e. no interested units for logging.
    #
    mon_unit_list = //


    # This setting can be used only when config parameter 'enable_mon_query' is 2.
    # Number of seconds between calls to SP that gathers monitoring data for all working attachments.
    # Monitor data will be gathered only by single (dedicated) isql session which is launched first.
    # Parameter 'sleep_ddl' must be uncommented and its value has to point on existent SQL script with UDF 
    # declaration that implements delay.
    # Actual duration of delay, in seconds, will be evaluated as minimal of 'mon_query_interval'
    # and test_time*60 divided by 20.
    #
    mon_query_interval = 60


    # How stock remainders should be verified BEFORE totalling will occur in sp_make_invnt_saldo
    # (declarative CHECK constraint on qty_xxx >= 0  should NOT ever be fired in this test!):
    # bit#0 := 1 ==> perform calls of SRV_FIND_QD_QS_MISM in doc_list_aiud in order
    #                to register mismatches between doc_data.qty and total number
    #                of rows in qdistr + qstorned for doc_data.id
    # bit#1 := 1 ==> perform calls of SRV_CHECK_NEG_REMAINDERS from doc_list_aiud
    #                (instead of totalling turnovers to `invnt_saldo` table)
    # bit#2 := 1 ==> allow dump dirty data into debug tables for analysis, see sp zdump4dbg, in case
    #                when PK/FK or check constraint is violated (see parameter 'halt_test_on_errors')
    #                NOTE: when bit#2 has value 1 then parameter 'create_with_debug_objects' must be 1
    #                to force build scenario create auxiliary Z-tables in the test DB.
    #
    qmism_verify_bitset = 1


    # Should 1st of being launched ISQL instances also start asynchronously FBSVCMGR with 
    # opening TRACE session (1=yes; 0=no; trace session will be stopped on every end of .sql) ?
    # If yes, final report will have result of parsing trace log for that ISQL activity with
    # aggregate data about each business action and average values of:
    # 1) speed of fetches and marks per second;
    # 2) ratios reads / fetches and writes / marks.
    # These values will be divided on 10 equal time intervals in order to see changes 
    # in performance that could occur during 'test_time' phase of test.
    # There is no performance penalty when single trace session is active so one may safely
    # to set this value to 1. Note though that if you will interrupt test by brute kill all
    # ISQL sessions than you have also to kill process of FBSVCMGR which can remain active.
    # Currently implemented only for Windows!
    #
    # trc_unit_perf = 0


#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS TO FORCEDLY TERMINATE OF WORK BEFORE THE DUE TIME
#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

    # Mnemonics of exceptions which forces test to be stopped (see calls of fn_halt_sign(gdscode)):
    # 'CK' -- halt if CHECK violation or 'not_valid' occurs (mostly this can be due to negative stock remainders)
    # 'PK' -- halt if PK or UK violation occurs
    # 'FK' -- halt if FK violation occurs // now n/a because test does not use foreign keys.
    # 'ST' -- halt if exc #335544842 appeared at the top of stack and logged into perf_log (strange problem only in 3.0 SC)
    # These mnemonics can be combined in list, i.e.: 'CK/PK/FK' - halt if CHECK or PK or FK violation occurs
    # Default: '/CK/' ==> force test to be stopped on attempt to write NEGATIVE values for stock remainders.
    # 12.02.2015: PK and FK violations *can* be detected only in sp_make_qty_storno & sp_kill_qty_storno,
    # but it is due to UNDEFINED order of UNDO when some Tx must perform bulk of such work.
    # Detailed investigation:
    # sql.ru/forum/1142271/posledstviya-nepredskazuemo-neposledovatelnyh-otkatov-izmeneniy-pri-exception
    # (EXPLANATION by dimitr see in e-mail, letters date = 12.02.2015)
    #
    halt_test_on_errors = /CK/
     

    # Parameter 'use_external_to_stop' defines name of text file that can be used for premature stop all working isql sessions.
    # This parameter is NOT required, i.e. it can be commented. In this case test can be stopped by running temporary script
    # '$tmpdir/1stoptest.tmp.sh' which is created every time when test starts by script '1run_oltp_emul.sh'.
    # Usage of this script is OK for most cases except extremely high workload when establishing of new connect will be unavaliable.
    #
    # When extremely high workload is used then following message can appear on every attempt to establish new attachment:
    #     Statement failed, SQLSTATE = 08004
    #     connection rejected by remote interface
    # This message will raise both in isql which tries to connect to any database or when you try to get server version byt invoking
    # fbsvcmgr host/port:service_mgr info_server_version -- i.e. when you try to connect to server security database.
    #
    # In such case it can be more reliable to use EXTERNAL TABLE (i.e. TEXT FILE) to make all attachments to stop their work. 
    # This is so because every isql performs code that 'looks' from time to time into this external table and checks existense of at 
    # least one record in it. So, test will be quickly self-stopped when at least one non-empty line exists there. 
    # Please note that you have to make this file EMPTY before every new test run.
    #
    # If you have decided to use EXTERNAL FILE following steps must be done for premature terminate all test activity:
    #   1. Open that file in text editor and type one ascii-character there;
    #   2. Press ENTER and save this file.
    #   3. Make this file empty again when all isql sessions terminated their work.
    # Also, please note on value of parameter "ExternalFileAccess" in firebird.conf:
    #   1. When ExternalFileAccess = FULL then 'use_external_to_stop' must be full path and name of text file that will be 
    #      queried by every attachment as 'stop flag'.
    #   2. When ExternalFileAccess = RESTRICTED then 'use_external_to_stop' must be only NAME of file, without path.
    # Default setting: UNDEFINED, i.e. only temporary batch '$tmpdir/1stoptest.tmp.sh' can be used for this.
    #
    # use_external_to_stop = stoptest.txt


#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR DATABASE CREATION PROCESS AND INITIAL DATA FILLING 
#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

    # *** NOTE *** 
    # These settings are ignored when DB already exists and has required number of documents.

    # Following two settings actual only when database does not exist 
    # and should be created by test script itself.
    # Valid options for create_with_fw are sync | async
    # Value of create_with_sweep should be not less than zero.
    #
    # RECOMMENDED value for create_with_sweep is 0 (zero).
    # Remember that sweep interval greater than 0 can lead to unpredictable affect on performance, especially for short test duration.
    #
    create_with_fw = sync
  
    create_with_sweep = 0


    # Should script be paused if database does not exist or its creation
    # did not finished properly (e.g. was interrupted; 1=yes; 0=no) ?
    # You have to set this parameter to 0 if this batch is launched by 
    # scheduler on regular basis. Otherwise it is recommended to set 1.
    #
    wait_if_not_exists = 0

    # Should script be PAUSED after creation database objects before starting
    # initial filling with <init_docs> documents (mostly need only for debug; 1=yes, 0=no) ?
    # NOTE: test database will be (re-)created only when it does not contain all necessary objects.
    # Command scenario verifies this by searching special record in table 'SEMAPHORES' which should
    # be added there at the end-point of building process. If this record is found, test will use 
    # existing database and following parameters will be IGNORED.
    #
    wait_after_create = 1

    # Number of documents, total of all types, for initial data population.
    # Command scenario will compare number of existing document with this
    # and create new ones only if <init_docs> still greater than obtained.
    #
    # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 0 (ZERO) ***
    #
    # Instead of generating dosuments by single attachment it is much more properly (and faster) to assign some big value 
    # to 'warm_time' parameter (say, 1440 which means to run for 1 day) and also set 'test_time' to 0 (zero). 
    # When size of database will reach value that you consider as enough then just stop the test by running batch 
    # '$tmpdir/1stoptest.tmp.sh' which always is created when test starts.
    #
    init_docs = 0


    # This parameter actual only when 'init_docs' greater than 0 and 'separate_workers' is 1, i.e. when you want to separate 
    # each ISQL session in such way that they will not ever meet update conflicts during work. In other cases value if this
    # parameter is ignored.
    # If you decide to generate initial quantity of documents by using old 'init_docs' value then assign to 'expected_workers' 
    # value that is equal to the  number of ISQL sessions that is expected to run.
    #
    expected_workers = 100


    # Actual only when 'init_docs' greater than 0 and FB mode is Classic Server or SuperClassic. Will be ignoired in SuperServer.
    # Number of pages for usage during init data population ("-c" switch for ISQL). Used ONLY during phase of initial data population.
    # Make sure that it is LESS than FileSystemCacheThreshold, which default is 65536.
    #
    # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 10000 ***
    #
    init_buff = 10000


    # Actual only when 'init_docs' greater than 0
    # Should command scenario (1run_oltp_emul.sh) be PAUSED after finish creating
    # required initial number of documents (see parameter 'init_docs'; 1=yes, 0=no) ?
    # Value = 1 can be set if you want to make copy of .fdb and restore later
    # this database to 'origin' state. This can save time because of avoiding need
    # to create <init_docs> again:
    #
    wait_for_copy = 1


    # Do we want to create some DEBUG objects (tables, views and procedures)
    # in order to:
    # 1) make dumps of all data from tables when critical error occurs;
    # 2) make miscelaneous diagnostic queries via "Z_" views.
    # Value=1 will cause "oltp_misc_debug.sql" be called when build database.
    # NB: setting 'QMISM_VERIFY_BITSET' must have bit #2 = 1 when this value = 1.
    # (see oltp_main_filling.sql)
    # Recommended value: 1
    #
    create_with_debug_objects = 1


    # Test has two tables which are subject of very intensive modifications: QDistr and QStorned.
    # Performance highly depends on time which engine spends on handling DP, PP and index pages
    # of this tables - they are "bottlenecks" of schema. Database can be created either with two
    # these tables or with several "clones" of them (with the same stucture). The latter allows
    # to "split" workload on different areas and reduce low-level lock contention.
    # Should heavy-loaded tables (QDistr and QStorned) be splitted on several different tables,
    # each one for separate pair of operations that are 'source' and 'target' of storning ?
    # Avaliable values: 
    # 0 = do NOT split workload on several tables (instead of single QDistr and QStorned);
    # 1 = USE several tables with the same structure in order to split heavy workload on them.
    #     NOTE (2019). Not only Qdistr and QStorned but also PERF_LOG table will be 'splitted' onto
    #     several tables (with names PERF_SPLIT_01...PERF_SPLIT_09) when this parameter is set to 1. 
    # Recommended value: 1.
    #
    create_with_split_heavy_tabs = 1

    # Whether heavy-loaded table (QDistr or its XQD_* clones) should have only one ("wide")
    # compound index or two separate indices (1=yes, 0=no).
    # Number of columns in compound index depends on value of two parameters:
    # 1) create_with_split_heavy_tabs and 2) create_with_separate_qdistr_idx (this).
    # Order of columns is defined by parameter 'create_with_compound_idx_selectivity'.
    # Recommended value: 0.
    #
    create_with_separate_qdistr_idx = 0

    # Parameter 'create_with_compound_columns_order' defines order of fields in the starting part
    # of compound index key for the table which is subject to most heavy workload - QDistr. 
    # Avaliable options: 'most_selective_first' or 'least_selective_first'.
    # When choice = 'most_selective_first' then first column of this index will have selectivity = 1 / <W>,
    # where <W> = number of rows in the table 'WARES', depends on selected workload mode.
    # Second and third columns will have poor selectivity = 1/6.
    # When choice = 'least_selective_first' then first and second columns will have poor selectivity = 1/6,
    # and third column will have selectivity = 1 / <W>.
    #
    # Actual only when create_with_split_heavy_tabs = 0.
    # Recommended value: most_selective_first
    #
    create_with_compound_columns_order = most_selective_first


#:::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR SCHEDULED-BASIS JOB AND TEST REPORT
#:::::::::::::::::::::::::::::::::::::::::::::::::::::

    # Number of intervals for splitting 'test_time' for "Performance in DYNAMIC" report which shows how performance did change in time.
    # NOTE. This report can be used only for quick and rough estimation only because it always will show small number of data.
    # Precise performance score will be show in "Performance per minute" report.
    # NOTE: TPC-C (rev 5.11, 5.6.4, page 78) requires that at least 240 intervals with maximum size 30 seconds will be used for such purpose.
    # If you want to see such detailed data then assign <test_intervals> = 2 * <test_time>, but much useful is report "Performance per minute".
    # Do not set this parameter value less than 30.
    #
    test_intervals = 30


    # Windows only. Not implemented for Linux, will be ignored at runtime.
    # Total number of downloaded snapshots for storing in the folder !tmpdir!\snapshots_archive.
    # Batch scenario will keep in this folder only newest snapshots and remove old ones.
    # This setting is useful for regression investigations: you will not need to rebuild FB from sources,
    # just replace last tested FB snapshot with some of old ones. 
    # If this parameter is undefined or has value of ZERO then snapshots will NOT be stored on disk.
    # Currently implemented only for Windows!
    #
    # max_snapshots_to_store=0

    # Windows only. Not implemented for Linux, will be ignored at runtime.
    # Do we want to replace existent Firebird instance every time before test will be launched
    # (by downloading binary archieve from official site, decompress it and stop/restart FB daemon).
    # Currently implemented only for Windows!
    #
    # replace_instance = 0

    # Windows only. Not implemented for Linux, will be ignored at runtime.
    # Used in "oltp-scheduled" scenario: name of etalon DB which serves as source 
    # for copy to work DB before every new test start:
    #
    # etalon_dbnm = /path/to/oltp25-etalone.fdb


    # Windows only. Not implemented for Linux, will be ignored at runtime.
    # Create report in HTML format (beside plain text one; 1=yes, 0=no) ?
    #
    make_html=0

    # Do we want to include into final report result of gathering database statistics (1=yes; 0=no) ?
    # WARNING! This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
    #
    run_db_statistics = 0

    # Do we want to include into final report result of database validation (1=yes; 0=no) ?
    # WARNING! This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
    #
    run_db_validation = 0

    # Should final report be saved in file with name which contain info about FB, database, test settings ?
    # If no, leave this parameter commented. In that case final report will be always saved with the same name.
    # If yes, choose format of this name:
    # regular   - appropriate for quick found performance degradation, without details of test settings
    # benchmark - appropriate for analysis when different settings are applied
    # Sample of report name when this parameter = 'regular':
    # 20151102_1448_score_06543_build_31236_ss25__3h00m_100_att_fw__on.txt
    # Sample of report name when this parameter = 'benchmark':
    # ss25_fw_off_split_most__sel_1st_one_index_score_06543_build_31236__3h00m_100_att_20151102_1448.txt
    #
    # Available options: regular | benchmark, or leave commented (undefined).
    #
    file_name_with_test_params = regular

    # Suffix for adding at the end of report name.
    # CHANGE this value to some useful info about host location, 
    # hardware specifics, FB instance etc.
    #
    file_name_this_host_info = no_host_info

    # Do we want to include in the report details about server hardware and OS (1 = yes, 0 = no) ?
    # NOTE: this setting has sense only when you launch ISQL sessions at the server which you are 
    # interesting on, i.e. when value of 'host' parameter is localhost or 127.0.0.1
    #
    gather_hardware_info = 1

    # When setting 'postie_send_args' is defined batch will send final report to required e-mail using console
    # client POSTIE.EXE with arguments that are defined here plus add auto generated subject and
    # attach report. This setting is OPTIONAL. Note: executable 'postie.exe' must be either in one
    #  of PATH-list or in ..\util related to current ('src') folder. Windows only.
    # Windows only. Not implemented for Linux, will be ignored at runtime.
    #
    #postie_send_args = -esmtp -host:mail.local -from:malert@company.com -to:foo@bar.com -user:malert@company.com -pass:QwerTyuI0p

    # Windows only. Should final report be uploaded (1=yes; 0 or undefined = no) ?
    #
    #upload_report = 0


#::::::::::::::::::::::::::::::::::::::::::::
#  EXOTIC SETTINGS (USAGE WAS NOT CHECKED)
#::::::::::::::::::::::::::::::::::::::::::::

    # Does Firebird running in embedded mode ? (1=yes, 0=no)
    #
    is_embed = 0

