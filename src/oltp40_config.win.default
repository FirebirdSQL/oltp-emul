####################################################################
# OLTP-EMUL test for Firebird database - configuration parameters.
# To get last version type following command:
# git clone https://github.com/FirebirdSQL/oltp-emul .
# This file is used for launching test and ISQL sessions on WINDOWS host
# with running Firebird 4.0.
# Parameters are extracted by '1run_oltp_emul.bat' command scenario.
####################################################################

#::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR START AND FINISH ISQL SESSIONS
#::::::::::::::::::::::::::::::::::::::::::::::::

    # Folder with Firebird console utilities (isql, fbsvcmgr).
    # Trailing backslash is optional.
    # Allows referencing to existing OS environment variable by using exclamation sign.
    # Samples:
    # fbc = C:\Firebird40
    #
    # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters!
    # If FB binaries are in directory like "C:\Program files (x86)\Firebird Database Server" 
    # then make copy of this folder to something like C:\fb40 etc.
    #
    fbc = C:\FB\40SS


    # Alias or full path and file name of database.
    # If you want this database be created by test itself, specify it as
    # FULL PATH and file name. Use only ASCII characters in its name.
    # Allows referencing to existing OS environment variable by using exclamation sign.
    # Samples:
    # dbnm = alias_oltp40
    # dbnm = c:\temp\oltp40.fdb
    #
    # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters.
    #
    dbnm = C:\FBTESTING\testdb\oltp40.fdb


    # Parameters for remote connection and authentication.
    # Will be ignored by command scenario if FB runs in embedded mode.

    host = localhost
    port = 3400
    usr =  SYSDBA
    pwd =  masterkey


    # Folder for storing .sql scenarios, STDOUT and STDERR logs of every working isql session.
    # Trailing backslash is optional.
    # Allows referencing to existing OS environment variable by using dollar sign.
    # Samples:
    # tmpdir = c:\temp\oltp-emul-40
    # tmpdir = !temp!\oltp-emul-40
    #
    # WARNING. DO NOT use names with spaces, parenthesis or non-ascii characters!
    # If your TEMP variable is like "C:\Documents and Settings\User\Local Settings\Temp"
    # then do not use here !temp! and specify here something like C:\TEMP instead.
    #
    tmpdir = c:\temp\logs.oltp40

    # Condition for removing or preserving isql logs after test finish.
    # Possible values: always | never | if_no_severe_errors
    # Option 'always' means that ISQL logs in %tmpdir% will be removed after test finish regardless any result.
    # Option 'never' means that ISQL logs will be always preserved.
    # Option 'if_no_severe_errors' means that ISQL logs will be removed only if no severe exceptions occured.
    # Test considers following exceptions as 'severe':
    #   335544558 check_constraint     (Operation violates CHECK constraint @1 on view or table @2).
    #   335544347 not_valid            (Validation error for column @1, value "@2").
    #   335544665 unique_key_violation (Violation of PRIMARY or UNIQUE KEY constraint "..." on table ...") - if table has unique CONSTRAINT
    #   335544349 no_dup               (attempt to store duplicate value (visible to active transactions) in unique index "***") - if table has only unique INDEX

    # Recommended value: if_no_severe_errors
    #
    remove_isql_logs = never


#:::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR REPLICATION, WORKLOAD LEVEL AND PAUSES
#:::::::::::::::::::::::::::::::::::::::::::

    # Test has several settings that define how much work should be done by each business action in average.
    # All of them are considered as separate enumerations: when new ISQL session creates connection, it reads
    # "entry" setting about selected workload level and then read all other settings for THIS workload level.
    # Parameter 'working_mode' is mnemonic these enumerations. Possible values for this parameter are:
    # SMALL_01, SMALL_02, SMALL_03, MEDIUM_01, MEDIUM_02, MEDIUM_03, LARGE_01, LARGE_02, LARGE_03 and HEAVY_01
    # In case of launching from several machines ensure that all of them have the same value for this parameter.
    # Completely new workload mode can be added to the test by editing file "oltp_main_filling.sql", see there
    # sub-section "Definitions for workload modes".
    # WARNING: exception will raise on test startup if this value was mistyped and has no correspond data in DB.
    # Mnemonic name of workload mode (must be specified without quotes, case-insensitive):
    #
    working_mode = small_03


    # Maximal number connections per second (max allowed rate of new attachments appearance for making workload grow smoothly).
    # We have to limit RATE of requests for new attachments, especially when total count of launching ISQL sessions 
    # is 1000 or more. Otherwise some of sessions will get failure on attempt to establish connection with text:
    #     Statement failed, SQLSTATE = 08004
    #     connection rejected by remote interface
    # If value of this parameter less than 10 or greater than 100 then delays will not occur and all sessions will try to establish
    # their attachments at the same time. This can be reason of "connection rejected" error.
    # Otherwise:
    # * if number of sessions not exceeds <max_cps> then delay will be evaluated as random value between 1 and 4 seconds.
    # * if number of sessions greater that this parameter then delay will be evaluated as: 1 + session_id / <max_cps>.
    # NOTE. If you intend to launch more than 1000 sessions then consider to adjust Windows registry settings:
    # * goto HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\TCPIP\Parameter
    # * create a new REG_DWORD value with name: TcpTimedWaitDelay. Set it to 60.
    # * create a new REG_DWORD value with name: MaxUserPort. Set it to 32768 (this defines ephemeral port range).
    #
    # Recommended value: 20...30
    #
    max_cps = 25


    # MINIMAL pause duration between each business operations, in seconds.
    # This parameter has default value 0 and can be commented.
    # Duration of pause will be evaluated at runtime as random value 
    # between %sleep_min% and %sleep_max% values.
    #
    sleep_min=0


    # MAXIMAL pause duration between each business operations, in seconds.
    # Default: 0 - no pauses, next transaction will start immediatelly after previous commit.
    # This leads to maximal (non-realistic) level of workload.
    # Delay statement will be inserted in .sql script by 1run_oltp_emul.bat; the form of this statement depends on value of
    # parameter 'sleep_ddl':
    # 1) If parameter 'sleep_ddl' is commented (undefined) then temporary .vbs script will be created in the %tmodir% folder
    #    and Windows built-in utility 'cscript.exe' will be invoked with passing required delay to this .vbs:
    #    cscript.exe //nologo //e:vbscript //t:NNN %tmpdir%\%tmp_vbs%
    #    This leads to excessive OS workload when number of sessions is more than ~300.
    # 2) Otherwise UDF will be invoked from separate execute block. Name of this UDF must be declared in the SQL-script defined 
    #    by 'sleep_ddl' parameter.
    # NOTE. THIS PARAMETER IS MANDATORY AND CAN NOT BE COMMENTED.  SPECIFY 0 IF NO PAUSES REQUIRED.
    #
    sleep_max=0

    # Name of SQL script with declaration of UDF that will be used for PAUSES between transactions.
    # An attachment will be IDLE only when <sleep_max> greater than 0 otherwise pauses will not occur at all.
    # Script must correctly drop old existent UDF with any name that contains phrases: DELAY, SLEEP or PAUSE.
    # After dropping, script also must create new UDF and test it (for checking results in log).
    # This script will be applied only when parameter 'sleep_max' greater than 0.
    # Note. The whole following phrase:
    #     declare external function <name>
    # -- must be written on the SINGLE LINE in this script (<name> will be parsed from this line as the 4th word)
    # This UDF implementation (.dll file) must be stored in the server-side folder, usually %FIREBIRD_HOME%\UDF, which
    # is allowed to be used for UDF calls. See description for parameter 'UDFaccess' in standard firebird.conf for details.
    # Test is provided with its own UDF and appropriate declaration script named "oltp_sleepUDF_win.sql".
    # Unpack file SleepUDF.dll.zip and put file SleepUDF.dll in any folder that is allowed by 'UDFaccess' parameter from firebird.conf.
    # Also, you can comment this parameter for using only external command ('shell cscript ...;') when 'sleep_max' greater than 0.
    # NOTE (FB 4.0+). You have to explicitly add "UdfAccess = Restrict UDF" line in your firebird.conf since FB 4.0 prohibits 
    # UDF usage by default.
    #
    sleep_ddl=.\oltp_sleepUDF_win.sql


    # Should SET TRANSACTION statement include NO AUTO UNDO clause (1=yes, 0=no) ?
    # Value 1 is useful when most of started transactions finished successfully.
    # Especially this can be so when setting 'separate_workers' has value 1.
    # Results of benchmark (2015):
    # SuperServer:  +  5-6%
    # SuperClassic: + 10-11%
    # Recommended value: 1
    #
    no_auto_undo = 1

    # Minimal interval in minutes between two subsequent calls of service procedure 'srv_recalc_idx_stat' which updates index statistics.
    # Only indexes for tables that are participated in most often performing queries are affected. 
    # Note that frequent update of index statistics has sense only for small databases which have quickly changed data distribution.
    # There is no sense to update index statistics if test will runs for 7-9 hours and database has size more than 100 Gb: most probably
    # it will finish at the moment when test itself will also be close to expiration. 
    # In that case set value of this parameter to zero to prevent selection of procedure that does this updating.
    #
    # Recommended value of this parameter depends on size od database:
    # within scope 30...60 minutes for databases with size up to 20 Gb;
    # within scope 60...90 minutes for databases with size 20...40 Gb;
    # within scope 90...120 minutes for databases with size 40...60 Gb;
    # within scope 120...240 minutes for databases with size 60...80 Gb;
    # 0 (zero) for databases with size more than 80...100 Gb (this means that statistics will not be updated at all).
    # NOTE. For big databases (with size more than 100 Gb) updating of index statistics has sense only for big <test_time> values.
    # See SP 'srv_random_unit_choice' for choising algorithm.
    #
    recalc_idx_min_interval = 0


    # If you plan this database work in replication process, one need to add PK constraints 
    # to all persistent tables that can be changed during test work.
    # Assign value of following parameter to 1 in order all necessary changes be added to tables DDL
    # (primary keys and triggers for some of tables).
    # Setting value to 0 will DROP all changes that are unneeded when test runs without replication.
    # This parameter can be changed 'on the fly': database recreation is NOT required, but in case
    # when database has valuable size, changes will be applied not instantly.
    #
    used_in_replication = 0


#:::::::::::::::::::::::::::::::::::
# SETTINGS RELATED TO LOCK CONFLICTS
#:::::::::::::::::::::::::::::::::::

    # Though test uses pessimistic locks (tries to 'catch' document before any further actions) this strategy can not prevent
    # neither from choosing the same document in random algorithm nor from decreasing stock remainder for the same wares.
    # Lot of update conflicts or exceptions related to violate CHECK CONSTRANT about non-negative remainder will occur if
    # we allow sessions to operate with the same ("shared") set of documents. This affects on performance, of course.
    # One may to separate sessions in such way that each of them will work within "sandbox" and never fall in conflict with
    # concurrent sessions for documents.
    # Assign 1 to this parameter if you want to separate work of sessions and thus totally exclude exceptions related to
    # update conflicts and violation or check constraint defined for aggregated remainders value.
    # Otherwise set it to 0.
    #
    separate_workers = 1

    # How many documents from other's sandbox can be taken in processing by 'this' ISQL session, percent.
    # Value 0 means that we do not allow ISQL session to take any documents except those which was created by itself.
    # Value 100 means that we require for each ISQL session take for processing only OTHER's documents. 
    # Moreover, this also mean that we want ISQL session 'forget' about documents which were created by itself.
    # This will lead to extremely high number of lock-conflicts and very poor performance.
    # Recommened value: 0 - for benchmark purposes; 30...50 - for investigations.
    #
    update_conflict_percent = 0

    # How business operations should be selected: randomly or in predictable manner.
    # Allowed values:
    # random - on occasional basis, but with respect to priority/probability of business operatrions nature;
    # predictable - forcely make every ISQL session to work using given sequence of business operations, i.e.
    #     create client order -> create order to supplier -> get invoice from supplier -> ...
    # Value 'predictable' was not deeply tested and currently can lead to poor performance.
    # Recommened value: random
    #
    unit_selection_method = random


#::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR ADDITIONAL LOGGING 
#::::::::::::::::::::::::::::::::::::


    # Do we add in ISQL logs detailed info for each actions that was registered while current transaction was performed ?
    # Note: value = 1 significantly increases disk I/O on client machine.
    # Do not use it if you are not interested on these data.
    # Recommended value: 0
    #
    detailed_info = 0


    # Setting for enabling queries to monitor tables in order to make detailed performance analysis.
    # When 0 then monitor tables are not queried.
    # When 1 then EVERY session will take two snapshots before and after execution of selected unit.
    # More detailed analysis with detalization down to separate stored procedures can be achieved
    # by updating setting 'mon_unit_list'.
    # When 2 then only ONE session is dedicated to gather monitoring data with obtaining data
    # that relates to ALL other working sessions.
    # It will call special SP 'srv_fill_mon_memo_consumption' every 'mon_query_interval'-th second.
    # NOTE: pauses for mon_unit_perf=2 between transactions will be done only when 'sleep_ddl' is defined.
    #
    mon_unit_perf = 0


    # This setting can be used only when config parameter 'enable_mon_query' is 1.
    # List of top-level units (see 'business_ops' table) which performance statistics we want 
    # to be logged by querying  monitoring tables. Logging is done by SP srv_log_mon_for_traced_units.
    # Value can be single unit name or LIST of unit names delimited by forward slash.
    # Sample:
    #     sp_make_qty_storno/sp_kill_qty_storno/sp_multiply_rows_for_qdistr/sp_multiply_rows_for_pdistr
    # Default value: two slashes without any characters between them, i.e. no interested units for logging.
    #
    mon_unit_list = //


    # This setting can be used only when config parameter 'enable_mon_query' is 2.
    # Number of seconds between calls to SP that gathers monitoring data for all working attachments.
    # Monitor data will be gathered only by single (dedicated) isql session which is launched first.
    # Parameter 'sleep_ddl' must be uncommented and its value has to point on existent SQL script with UDF 
    # declaration that implements delay.
    # Actual duration of delay, in seconds, will be evaluated as minimal of 'mon_query_interval'
    # and test_time*60 divided by 20.
    #
    mon_query_interval = 60


    # Mnemonics of exceptions which forces test to be stopped (see calls of fn_halt_sign(gdscode)):
    # 'CK' -- halt if CHECK violation or 'not_valid' occurs (mostly this can be due to negative stock remainders)
    # 'PK' -- halt if PK or UK violation occurs
    # 'FK' -- halt if FK violation occurs // now n/a because test does not use foreign keys.
    # 'ST' -- halt if exc #335544842 appeared at the top of stack and logged into perf_log (strange problem only in 3.0 SC)
    # These mnemonics can be combined in list, i.e.: 'CK/PK/FK' - halt if CHECK or PK or FK violation occurs
    # Default: '/CK/' ==> force test to be stopped on attempt to write NEGATIVE values for stock remainders.
    # 12.02.2015: PK and FK violations *can* be detected only in sp_make_qty_storno & sp_kill_qty_storno,
    # but it is due to UNDEFINED order of UNDO when some Tx must perform bulk of such work.
    # Detailed investigation:
    # sql.ru/forum/1142271/posledstviya-nepredskazuemo-neposledovatelnyh-otkatov-izmeneniy-pri-exception
    # (EXPLANATION by dimitr see in e-mail, letters date = 12.02.2015)
    #
    halt_test_on_errors = /CK/
     

    # How stock remainders should be verified BEFORE totalling will occur in sp_make_invnt_saldo
    # (declarative CHECK constraint on qty_xxx >= 0  should NOT ever be fired in this test!):
    # bit#0 := 1 ==> perform calls of SRV_FIND_QD_QS_MISM in doc_list_aiud in order
    #                to register mismatches between doc_data.qty and total number
    #                of rows in qdistr + qstorned for doc_data.id
    # bit#1 := 1 ==> perform calls of SRV_CHECK_NEG_REMAINDERS from doc_list_aiud
    #                (instead of totalling turnovers to `invnt_saldo` table)
    # bit#2 := 1 ==> allow dump dirty data into debug tables for analysis, see sp zdump4dbg, in case
    #                when PK/FK or check constraint is violated (see parameter 'halt_test_on_errors')
    #                NOTE: when bit#2 has value 1 then parameter 'create_with_debug_objects' must be 1
    #                to force build scenario create auxiliary Z-tables in the test DB.
    #
    qmism_verify_bitset = 1


    # Should 1st of being launched ISQL instances also start asynchronously FBSVCMGR with 
    # opening TRACE session (1=yes; 0=no; trace session will be stopped on every end of .sql) ?
    # If yes, final report will have result of parsing trace log for that ISQL activity with
    # aggregate data about each business action and average values of:
    # 1) speed of fetches and marks per second;
    # 2) ratios reads / fetches and writes / marks.
    # These values will be divided on 10 equal time intervals in order to see changes 
    # in performance that could occur during 'test_time' phase of test.
    # There is no performance penalty when single trace session is active so one may safely
    # to set this value to 1. Note though that if you will interrupt test by brute kill all
    # ISQL sessions than you have also to kill process of FBSVCMGR which can remain active.
    # Currently implemented only for Windows!
    #
    trc_unit_perf = 0


#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR TEST DURATION AND PREMATURE WORK CANCELLATION
#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

    # Number of minutes since test launch for which evaluation of performance score is omited because database is 'cold' (not in cache).
    # Means the same as 'ramp-up' period in TPC-C specification: we have to allow all sessions to establish  attachments and read some
    # data into Firebird page cache.
    # Recommended value: DBSize_Gb/2, where DBSize_Gb is size of database in Gb, but not less than 30 minutes.
    # You can estimate whether value of this parameter is apropriate if run test for 2-3 hours and look after its finish in report 
    # "Performance per minute". Performance counter at the end of <warm_time> period must be close to values for subsequent 20-30 minutes.
    # See also TPC-C specification rev 5.11:
    # * 5.6.4 (page 78) - graphical explanation of ramp-up period;
    # * Appendix C (page 132) - numerical quantities summary.
    #
    warm_time = 30


    # Duration of main test phase which starts after 'ramp-up'. Means the same as 'measurement interval' in TPC-C specification.
    # Overall performance score is evaluated as total number of successfully completed transactions during this phase divided by <test_time>.
    # At the end of this phase test will stop itself, i.e. you do not have to interrupt ISQL sessions.
    # Note that TPC-C requires minimum 120 minutes for this phase, but your system must allows to run test during 480 minutes - and this
    # value does not include <warm_time> phase (see TPC-C rev 5.1, 5.5.2.1, page 75).
    # ATTENTION. Reports and performance score for test_time less than 120 minutes must be considered as unreliable (doubtful).
    # Recommended value: at least 180.
    #
    test_time = 180


    # Number of intervals for splitting 'test_time' for "Performance in DYNAMIC" report which shows how performance did change in time.
    # NOTE. This report can be used only for quick and rough estimation only because it always will show small number of data.
    # Precise performance score will be show in "Performance per minute" report.
    # NOTE: TPC-C (rev 5.11, 5.6.4, page 78) requires that at least 240 intervals with maximum size 30 seconds will be used for such purpose.
    # If you want to see such detailed data then assign <test_intervals> = 2 * <test_time>, but much useful is report "Performance per minute".
    # Do not set this parameter value less than 30.
    #
    test_intervals = 30


    # Parameter 'use_external_to_stop' defines name of text file that can be used for premature stop all working isql sessions.
    # This parameter is NOT required, i.e. it can be commented. In this case test can be stopped by running temporary script
    # '%tmpdir%\1stoptest.tmp.bat' which is created every time at test start phase by script '1run_oltp_emul.bat'.
    # Usage of this script is OK for most cases except extremely high workload when establishing of new connect will be unavaliable.
    #
    # When extremely high workload is used then following message can appear on every attempt to establish new attachment:
    #     Statement failed, SQLSTATE = 08004
    #     connection rejected by remote interface
    # This message will raise both in isql which tries to connect to any database or when you try to get server version byt invoking
    # fbsvcmgr host/port:service_mgr info_server_version -- i.e. when you try to connect to server security database.
    #
    # In such case it can be more reliable to use EXTERNAL TABLE (i.e. TEXT FILE) to make all attachments to stop their work. 
    # This is so because every isql performs code that 'looks' from time to time into this external table and checks existense of at 
    # least one record in it. So, test will be quickly self-stopped when at least one non-empty line exists there. 
    # Please note that you have to make this file EMPTY before every new test run.
    #
    # If you have decided to use EXTERNAL FILE following steps must be done for premature terminate all test activity:
    #   1. Open that file in text editor and type one ascii-character there;
    #   2. Press ENTER and save this file.
    #   3. Make this file empty again when all isql sessions terminated their work.
    # Also, please note on value of parameter "ExternalFileAccess" in firebird.conf:
    #   1. When ExternalFileAccess = FULL then 'use_external_to_stop' must be full path and name of text file that will be 
    #      queried by every attachment as 'stop flag'.
    #   2. When ExternalFileAccess = RESTRICTED then 'use_external_to_stop' must be only NAME of file, without path.
    # Default setting: UNDEFINED, i.e. temporary batch '1stoptest.tmp.bat' in %tmpdir% folder can be used for this.

    # use_external_to_stop = c:\temp\stoptest.txt
    # use_external_to_stop = stoptest.txt


#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR DATABASE CREATION PROCESS AND INITIAL DATA FILLING 
#::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

    # *** NOTE *** 
    # These settings are ignored when DB already exist and has required number of documents.

    # Following two settings actual only when database does not exist 
    # and should be created by test script itself.
    # Valid options for create_with_fw are sync | async
    # Value of create_with_sweep should be not less than zero.
    #
    # RECOMMENDED value for create_with_sweep is 0 (zero).
    # Remember that sweep interval greater than 0 can lead to unpredictable affect on performance, especially for short test duration.
    #
    create_with_fw = async
  
    create_with_sweep = 0


    # Should script be paused if database does not exist or its creation
    # did not finished properly (e.g. was interrupted; 1=yes; 0=no) ?
    # You have to set this parameter to 0 if this batch is launched by 
    # scheduler on regular basis. Otherwise it is recommended to set 1.
    #
    wait_if_not_exists = 0

    # Should script be PAUSED after creation database objects before starting
    # initial filling with <init_docs> documents (mostly need only for debug; 1=yes, 0=no) ?
    # NOTE: test database will be (re-)created only when it does not contain all necessary objects.
    # Command scenario verifies this by searching special record in table 'SEMAPHORES' which should
    # be added there at the end-point of building process. If this record is found, test will use 
    # existing database and following parameters will be IGNORED.
    #
    wait_after_create = 1

    # Number of documents, total of all types, for initial data population.
    # Command scenario will compare number of existing document with this
    # and create new ones only if <init_docs> still greater than obtained.
    #
    # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 0 (ZERO) ***
    #
    # Instead of generating dosuments by single attachment it is much more properly (and faster) to assign some big value 
    # to 'warm_time' parameter (say, 1440 which means to run for 1 day) and also set 'test_time' to 0 (zero). 
    # When size of database will reach value that you consider as enough then just stop the test by running batch 
    # '%tmpdir%\1stoptest.tmp.bat' which always is created when test starts.
    #
    init_docs = 0


    # This parameter actual only when 'init_docs' greater than 0 and 'separate_workers' is 1, i.e. when you want to separate 
    # each ISQL session in such way that they will not ever meet update conflicts during work. In other cases value if this
    # parameter is ignored.
    # If you decide to generate initial quantity of documents by using old 'init_docs' value then assign to 'expected_workers' 
    # value that is equal to the  number of ISQL sessions that is expected to run.
    #
    expected_workers = 100


    # Actual only when 'init_docs' greater than 0 and FB mode is Classic Server or SuperClassic. Will be ignoired in SuperServer.
    # Number of pages for usage during init data population ("-c" switch for ISQL). Used ONLY during phase of initial data population.
    # Make sure that it is LESS than FileSystemCacheThreshold, which default is 65536.
    #
    # *** NOTE *** THIS VALUE IS OBSOLETE, LEAVE IT EQUAL TO 10000 ***
    #
    init_buff = 10000


    # Actual only when 'init_docs' greater than 0
    # Should command scenario (1run_oltp_emul.bat) be PAUSED after finish creating
    # required initial number of documents (see parameter 'init_docs'; 1=yes, 0=no) ?
    # Value = 1 can be set if you want to make copy of .fdb and restore later
    # this database to 'origin' state. This can save time because of avoiding need
    # to create <init_docs> again:
    #
    wait_for_copy = 1


    # Do we want to create some DEBUG objects (tables, views and procedures)
    # in order to:
    # 1) make dumps of all data from tables when critical error occurs;
    # 2) make miscelaneous diagnostic queries via "Z_" views.
    # Value=1 will cause "oltp_misc_debug.sql" be called when build database.
    # NB: setting 'QMISM_VERIFY_BITSET' must have bit #2 = 1 when this value = 1.
    # (see oltp_main_filling.sql)
    # Recommended value: 1
    #
    create_with_debug_objects = 1


    # Test has two tables which are subject of very intensive modifications: QDistr and QStorned.
    # Performance highly depends on time which engine spends on handling DP, PP and index pages
    # of this tables - they are "bottlenecks" of schema. Database can be created either with two
    # these tables or with several "clones" of them (with the same stucture). The latter allows
    # to "split" workload on different areas and reduce low-level lock contention.
    # Should heavy-loaded tables (QDistr and QStorned) be splitted on several different tables,
    # each one for separate pair of operations that are 'source' and 'target' of storning ?
    # Avaliable values: 
    # 0 = do NOT split workload on several tables (instead of single QDistr and QStorned);
    # 1 = USE several tables with the same structure in order to split heavy workload on them.
    #     NOTE (2019). Not only Qdistr and QStorned but also PERF_LOG table will be 'splitted' onto
    #     several tables (with names PERF_SPLIT_01...PERF_SPLIT_09) when this parameter is set to 1. 
    # Recommended value: 1.
    #
    create_with_split_heavy_tabs = 1

    # Whether heavy-loaded table (QDistr or its XQD_* clones) should have only one ("wide")
    # compound index or two separate indices (1=yes, 0=no).
    # Number of columns in compound index depends on value of two parameters:
    # 1) create_with_split_heavy_tabs and 2) create_with_separate_qdistr_idx (this).
    # Order of columns is defined by parameter 'create_with_compound_idx_selectivity'.
    # Recommended value: 0.
    #
    create_with_separate_qdistr_idx = 0

    # Parameter 'create_with_compound_columns_order' defines order of fields in the starting part
    # of compound index key for the table which is subject to most heavy workload - QDistr. 
    # Avaliable options: 'most_selective_first' or 'least_selective_first'.
    # When choice = 'most_selective_first' then first column of this index will have selectivity = 1 / <W>,
    # where <W> = number of rows in the table 'WARES', depends on selected workload mode.
    # Second and third columns will have poor selectivity = 1/6.
    # When choice = 'least_selective_first' then first and second columns will have poor selectivity = 1/6,
    # and third column will have selectivity = 1 / <W>.
    #
    # Actual only when create_with_split_heavy_tabs = 0.
    # Recommended value: most_selective_first
    #
    create_with_compound_columns_order = most_selective_first


#:::::::::::::::::::::::::::::::::::::::::::::::::::::
#  SETTINGS FOR SCHEDULED-BASIS JOB AND TEST REPORT
#:::::::::::::::::::::::::::::::::::::::::::::::::::::

    # Total number of downloaded snapshots for storing in the folder !tmpdir!\snapshots_archive.
    # Batch scenario will keep in this folder only newest snapshots and remove old ones.
    # This setting is useful for regression investigations: you will not need to rebuild FB from sources,
    # just replace last tested FB snapshot with some of old ones. 
    # If this parameter is undefined or has value of ZERO then snapshots will NOT be stored on disk.
    # Currently implemented only for Windows!
    #
    # max_snapshots_to_store=0

    # Do we want to replace existent Firebird instance every time before test will be launched
    # (by downloading binary archieve from official site, decompress it and stop/restart FB daemon).
    # Currently implemented only for Windows!
    #
    # replace_instance = 0

    # Used in `oltp-scheduled.bat`: name of etalon DB which serves as source 
    # for copy to work DB before every new test start:
    #
    # etalon_dbnm = c:\path\to\oltp40-etalone.fdb


    # Create report in HTML format (beside plain text one; 1=yes, 0=no) ?
    # Windows only. Not implemented for Linux, will be ignored at runtime.
    #
    make_html=0

    # Do we want to include into final report result of gathering database statistics (1=yes; 0=no) ?
    # WARNING! This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
    #
    run_db_statistics = 0

    # Do we want to include into final report result of database validation (1=yes; 0=no) ?
    # WARNING! This operation can take lot of time on big databases. Replace this setting with 0 for skip this action.
    #
    run_db_validation = 0

    # Should final report be saved in file with name which contain info about FB, database, test settings ?
    # If no, leave this parameter commented. In that case final report will be always saved with the same name.
    # If yes, choose format of this name:
    # regular   - appropriate for quick found performance degradation, without details of test settings
    # benchmark - appropriate for analysis when different settings are applied
    # Sample of report name when this parameter = 'regular':
    # 20151102_1448_score_06543_build_31236_ss40__3h00m_100_att_fw__on.txt
    # Sample of report name when this parameter = 'benchmark':
    # ss40_fw_off_split_most__sel_1st_one_index_score_06543_build_31236__3h00m_100_att_20151102_1448.txt
    #
    # Available options: regular | benchmark, or leave commented (undefined).
    #
    file_name_with_test_params = regular

    # Suffix for adding at the end of report name.
    # CHANGE this value to some useful info about host location, 
    # hardware specifics, FB instance etc.
    #
    file_name_this_host_info = no_host_info


    # Do we want to include in the report details about server hardware and OS (1 = yes, 0 = no) ?
    # NOTE: this setting has sense only when you launch ISQL sessions at the server which you are 
    # interesting on, i.e. when value of 'host' parameter is localhost or 127.0.0.1
    #
    gather_hardware_info = 1

    
    # When setting 'postie_send_args' is defined batch will send final report to required e-mail using console
    # client POSTIE.EXE with arguments that are defined here plus add auto generated subject and
    # attach report. This setting is OPTIONAL. Note: executable 'postie.exe' must be either in one
    #  of PATH-list or in ..\util related to current ('src') folder. Windows only.
    # Windows only. Not implemented for Linux, will be ignored at runtime.
    #
    #postie_send_args = -esmtp -host:mail.local -from:malert@company.com -to:foo@bar.com -user:malert@company.com -pass:QwerTyuI0p

    # Windows only. Should final report be uploaded (1=yes; 0 or undefined = no) ?
    # If no, leave this parameter commented. In that case final report will be preserved on local drive, in the folder %tmpdir%.
    # If yes, set its value to 1 and make ensure that batch ..\util\upload.bat has command that is able to upload text html content
    # to external source and store its result in text log with marking successful finish with phrase containing word 'success'.
    # One of such console utility can be found here: http://curl.haxx.se/download.html
    # Sample of its usage: curl.exe -F "name=%1" -F "file=@%2" http://some_external_site_as_storage
    #
    #upload_report = 0


#::::::::::::::::::::::::::::::::::::::::::::
#  EXOTIC SETTINGS (USAGE WAS NOT CHECKED)
#::::::::::::::::::::::::::::::::::::::::::::

    # Do we use mtee.exe utility to provide timestamps for error messages 
    # before they are logged in .err files (1=yes, 0=no) ?
    # Windows only. Not implemented for Linux, will be ignored at runtime.
    #
    use_mtee = 0

    # Does Firebird running in embedded mode ? (1=yes, 0=no)
    #
    is_embed = 0

